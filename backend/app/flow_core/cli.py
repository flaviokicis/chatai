from __future__ import annotations

import argparse
import json
import os
from pathlib import Path
from uuid import UUID

from dotenv import load_dotenv
from langchain.chat_models import init_chat_model

from app.core.langchain_adapter import LangChainToolsLLM
from app.db.repository import get_active_tenants, get_flows_by_tenant
from app.db.session import db_session
from app.services.tenant_config_service import ProjectContext, TenantConfigService

from .compiler import compile_flow
from .ir import Flow
from .runner import FlowTurnRunner
from .state import FlowContext


def _playground_flow_path() -> Path:
    """Return backend/playground/flow_example.json resolved from this file."""
    # __file__ = backend/app/flow_core/cli.py -> parents[2] = backend
    backend_dir = Path(__file__).resolve().parents[2]
    return backend_dir / "playground" / "flow_example.json"


class SimpleCLIAdapter:
    """Simple CLI adapter to replace the deleted CLIAdapter."""
    
    def __init__(self):
        pass
    
    def print_message(self, text: str) -> None:
        """Print a message to the console."""
        print(f"ðŸ¤– {text}")
    
    def print_messages(self, messages: list[dict[str, any]]) -> None:
        """Print multiple messages with delays."""
        for msg in messages:
            delay_ms = msg.get("delay_ms", 0)
            if delay_ms > 0:
                print(f"   [delay: {delay_ms}ms]")
            self.print_message(msg["text"])
    
    def get_user_input(self, prompt: str = "You: ") -> str:
        """Get user input from the console."""
        try:
            return input(prompt).strip()
        except KeyboardInterrupt:
            print("\nGoodbye!")
            exit(0)
    
    def print_status(self, message: str) -> None:
        """Print a status message."""
        print(f"ðŸ“Š {message}")
    
    def print_error(self, message: str) -> None:
        """Print an error message."""
        print(f"âŒ {message}")


def run_cli() -> None:
    parser = argparse.ArgumentParser(description="Run a Flow IR JSON interactively")
    parser.add_argument(
        "json_path",
        nargs="?",
        type=Path,
        default=_playground_flow_path(),
        help="Path to flow JSON file (default: backend/playground/flow_example.json)",
    )
    parser.add_argument("--llm", action="store_true", help="Use LLM to fill answers")
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-5",
        help="Chat model for tool calling (default: gpt-5)",
    )
    parser.add_argument(
        "--rewrite-model",
        type=str,
        default="gpt-5",
        help="Model used for rewrites (not used in CLI, reserved)",
    )
    parser.add_argument(
        "--no-rewrite",
        action="store_true",
        help="Disable conversational rewriting (raw GPT responses)",
    )
    parser.add_argument(
        "--tenant",
        type=str,
        help="Tenant ID to load flows from database",
    )

    args = parser.parse_args()

    # Load environment variables
    load_dotenv()

    # Initialize CLI adapter
    cli_adapter = SimpleCLIAdapter()

    # Initialize LLM if requested
    llm_client = None
    if args.llm:
        try:
            # Set up API keys based on model
            if args.model.startswith("gpt"):
                if not os.getenv("OPENAI_API_KEY"):
                    cli_adapter.print_error("OPENAI_API_KEY environment variable required for GPT models")
                    return
                chat_model = init_chat_model(args.model, model_provider="openai")
            else:
                if not os.getenv("GOOGLE_API_KEY"):
                    cli_adapter.print_error("GOOGLE_API_KEY environment variable required for non-OpenAI models")
                    return
                chat_model = init_chat_model(args.model, model_provider="google")
            
            llm_client = LangChainToolsLLM(chat_model)
            cli_adapter.print_status(f"LLM initialized: {args.model}")
        except Exception as e:
            cli_adapter.print_error(f"Failed to initialize LLM: {e}")
            return

    # Load flow
    flow_json = None
    compiled_flow = None
    
    if args.tenant:
        # Load from database
        cli_adapter.print_status(f"Loading flows from database for tenant: {args.tenant}")
        try:
            with db_session() as session:
                tenants = get_active_tenants(session)
                tenant_found = False
                for tenant in tenants:
                    if tenant.id == args.tenant:
                        tenant_found = True
                        flows = get_flows_by_tenant(session, tenant.id)
                        if flows:
                            # Use the first flow for now
                            flow_data = flows[0]
                            flow_json = flow_data.definition
                            cli_adapter.print_status(f"Loaded flow: {flow_data.name} (ID: {flow_data.id})")
                        else:
                            cli_adapter.print_error(f"No flows found for tenant {args.tenant}")
                            return
                        break
                
                if not tenant_found:
                    cli_adapter.print_error(f"Tenant '{args.tenant}' not found")
                    return
        except Exception as e:
            cli_adapter.print_error(f"Failed to load flow from database: {e}")
            return
    else:
        # Load from JSON file
        if not args.json_path.exists():
            cli_adapter.print_error(f"Flow file not found: {args.json_path}")
            return

        try:
            with open(args.json_path, "r", encoding="utf-8") as f:
                flow_json = json.load(f)
            cli_adapter.print_status(f"Loaded flow from: {args.json_path}")
        except Exception as e:
            cli_adapter.print_error(f"Failed to load flow file: {e}")
            return

    # Compile flow
    try:
        flow = Flow.model_validate(flow_json)
        compiled_flow = compile_flow(flow)
        flow_name = flow.metadata.name if flow.metadata else compiled_flow.id
        cli_adapter.print_status(f"Flow compiled successfully: {flow_name}")
    except Exception as e:
        cli_adapter.print_error(f"Failed to compile flow: {e}")
        return

    # Initialize runner
    if not llm_client:
        cli_adapter.print_error("LLM is required for the current flow system. Use --llm flag.")
        return
    
    runner = FlowTurnRunner(compiled_flow, llm_client)
    context = runner.initialize_context()
    
    # Project context (optional)
    project_context = None
    if args.tenant:
        try:
            with db_session() as session:
                config_service = TenantConfigService()
                tenant_config = config_service.get_tenant_config(args.tenant, session)
                if tenant_config and tenant_config.project_context:
                    project_context = ProjectContext.model_validate(tenant_config.project_context)
                    cli_adapter.print_status("Loaded project context from tenant config")
        except Exception as e:
            cli_adapter.print_status(f"Could not load project context: {e}")

    # Main conversation loop
    cli_adapter.print_status("Starting conversation. Type 'quit' or 'exit' to stop, Ctrl+C to interrupt.")
    cli_adapter.print_status(f"Flow: {flow_name}")
    if project_context:
        cli_adapter.print_status(f"Project: {project_context.name}")
    print()

    turn_count = 0
    while True:
        turn_count += 1
        
        # Get user input
        if turn_count == 1:
            # First turn - start the conversation
            user_input = cli_adapter.get_user_input("Start conversation: ")
        else:
            user_input = cli_adapter.get_user_input()
        
        if user_input.lower() in ["quit", "exit", "bye"]:
            cli_adapter.print_status("Goodbye!")
            break
        
        if not user_input:
            continue

        # Process the turn
        try:
            result = runner.process_turn(context, user_input, project_context)
            
            # Display response
            if result.messages:
                cli_adapter.print_messages(result.messages)
            elif result.assistant_message:
                cli_adapter.print_message(result.assistant_message)
            
            # Show debug info
            if result.tool_name:
                print(f"   [Tool: {result.tool_name}]")
            if result.confidence < 1.0:
                print(f"   [Confidence: {result.confidence:.2f}]")
            if result.reasoning:
                print(f"   [Reasoning: {result.reasoning}]")
            
            # Update context
            context = result.ctx if hasattr(result, 'ctx') else context
            
            # Check for completion
            if result.terminal:
                cli_adapter.print_status("ðŸŽ‰ Flow completed!")
                if context.answers:
                    print("\nðŸ“‹ Final answers:")
                    for key, value in context.answers.items():
                        print(f"  {key}: {value}")
                break
            
            # Check for escalation
            if result.escalate:
                cli_adapter.print_status("ðŸš¨ Escalated to human agent")
                break
                
        except KeyboardInterrupt:
            cli_adapter.print_status("Interrupted by user")
            break
        except Exception as e:
            cli_adapter.print_error(f"Error processing turn: {e}")
            # Continue the conversation instead of breaking
            continue

    cli_adapter.print_status("Session ended")


if __name__ == "__main__":
    run_cli()