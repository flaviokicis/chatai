from __future__ import annotations

from typing import TYPE_CHECKING

from .prompt_logger import prompt_logger

if TYPE_CHECKING:  # avoid hard import at runtime
    from app.services.tenant_config_service import ProjectContext

    from .llm import LLMClient

# Human pacing defaults for multi-message outputs
MIN_FOLLOWUP_DELAY_MS = 2200
MAX_FOLLOWUP_DELAY_MS = 4000
MAX_MULTI_MESSAGES = 8

DEFAULT_INSTRUCTION = (
    "CONTEXTO: VocÃª Ã© uma recepcionista brasileira profissional mas calorosa, reescrevendo mensagens para WhatsApp.\n\n"
    "TOM IDEAL - RECEPCIONISTA BRASILEIRA:\n"
    "- Profissional mas nÃ£o fria\n"
    "- Calorosa mas nÃ£o excessivamente casual\n" 
    "- Natural mas nÃ£o gÃ­ria demais\n"
    "- SimpÃ¡tica mas mantÃ©m respeito\n\n"
    "ANÃLISE CONTEXTUAL CRÃTICA:\n"
    "1. Se NÃƒO hÃ¡ mensagem do usuÃ¡rio ou Ã© primeira interaÃ§Ã£o â†’ Seja acolhedora: 'OlÃ¡! Como posso ajudar?'\n"
    "2. Se o usuÃ¡rio deu uma RESPOSTA/INFORMAÃ‡ÃƒO â†’ ReconheÃ§a adequadamente: 'Perfeito!', 'Ã“timo!', 'Entendi!'\n"
    "3. Se o usuÃ¡rio fez SAUDAÃ‡ÃƒO ('oi', 'olÃ¡') â†’ Responda Ã  saudaÃ§Ã£o: 'Oi! Tudo bem?', 'OlÃ¡!'\n"
    "4. Se o usuÃ¡rio fez PERGUNTA â†’ Seja prestativa sem reconhecimentos desnecessÃ¡rios\n\n"
    "TRANSFORMAÃ‡ÃƒO:\n"
    "Reescreva em UMA frase que:\n"
    "- Soe como uma recepcionista brasileira experiente e querida\n"
    "- Mantenha 100% do significado original\n"
    "- Use linguagem natural mas adequada ao contexto profissional\n"
    "- APENAS conecte com a mensagem anterior SE fizer sentido contextual\n"
    "- Tenha personalidade sem perder a cordialidade profissional\n\n"
    "VARIEDADE PROFISSIONAL:\n"
    "- EVITE reconhecimentos genÃ©ricos quando nÃ£o fazem sentido\n"
    "- Varie inÃ­cios CONFORME O CONTEXTO: reconhecimento, saudaÃ§Ã£o, ou pergunta direta\n"
    "- Use formas cordiais: 'me conta', 'pode me dizer', 'qual seria', 'como funciona'\n"
    "- Evite gÃ­rias muito casuais: nÃ£o 'tipo', 'E aÃ­', 'qual que Ã©'\n\n"
    "NATURALIDADE PROFISSIONAL:\n"
    "- Use contraÃ§Ãµes naturais: 'tÃ¡', 'pra', 'nÃ©', mas com moderaÃ§Ã£o\n"
    "- Finalize cordialmente: '?' simples, 'certo?', ou sem nada\n"
    "- Evite tanto formalidade excessiva quanto casualidade demais\n"
    "- Tom: como uma recepcionista que vocÃª adora conversar mas que Ã© competente\n\n"
    "LEMBRE-SE: Uma frase sÃ³, profissional mas calorosa, CONTEXTUALMENTE APROPRIADA."
)


def naturalize_prompt(
    llm: LLMClient,  # type: ignore[name-defined]
    text: str,
    instruction: str | None = None,
    project_context: ProjectContext | None = None,  # type: ignore[name-defined]
    user_message: str | None = None,
    conversation_context: list[dict[str, str]] | None = None,
) -> str:
    """
    Naturalize a prompt for WhatsApp with optional project context and user context.

    Args:
        llm: LLM client for rewriting
        text: Original text to rewrite
        instruction: Custom instruction (overrides default)
        project_context: Business context for better communication style
        user_message: The user's last message for context
        conversation_context: Recent conversation history for context
    """
    instr = instruction or DEFAULT_INSTRUCTION

    # Add project context to instruction if available
    if project_context and project_context.has_rewriter_context():
        context_prompt = project_context.get_rewriter_context_prompt()
        instr = f"{instr}\n{context_prompt}"

    # Build context-aware input for the LLM
    llm_input = f"Texto para naturalizar: {text}"
    
    if user_message:
        llm_input += f"\n\nÃšltima mensagem do usuÃ¡rio: {user_message}"
    
    if conversation_context:
        recent_msgs = conversation_context[-3:]  # Last 3 messages for context
        context_str = "\n".join(f"{msg.get('role', 'unknown')}: {msg.get('content', '')}" for msg in recent_msgs)
        llm_input += f"\n\nContexto da conversa:\n{context_str}"

    try:
        rewritten = llm.rewrite(instr, llm_input)
        
        # Log the prompt and response
        prompt_logger.log_prompt(
            prompt_type="naturalize_single",
            instruction=instr,
            input_text=llm_input,
            response=rewritten if isinstance(rewritten, str) else str(rewritten),
            model=getattr(llm, 'model_name', 'unknown'),
            metadata={"has_project_context": project_context is not None}
        )
        
        if isinstance(rewritten, str) and rewritten.strip():
            # Defensive sanitization to ensure single-line question
            first_line = next((ln.strip() for ln in rewritten.splitlines() if ln.strip()), "")
            if first_line.startswith(("- ", "* ")):
                first_line = first_line[2:].strip()
            if first_line.startswith('"') and first_line.endswith('"') and len(first_line) > 1:
                first_line = first_line[1:-1].strip()
            return first_line or text
        return text
    except Exception as e:
        # Log the error too
        prompt_logger.log_prompt(
            prompt_type="naturalize_single_error",
            instruction=instr,
            input_text=llm_input,
            response=f"ERROR: {e}",
            model=getattr(llm, 'model_name', 'unknown'),
            metadata={"error": str(e)}
        )
        return text


def clarify_and_reask(
    llm: LLMClient,  # type: ignore[name-defined]
    question_text: str,
    user_message: str,
    project_context: ProjectContext | None = None,  # type: ignore[name-defined]
) -> str:
    """
    Produce a brief, casual PT-BR acknowledgement referencing the user's words,
    then restate the original question succinctly in one sentence.

    Args:
        llm: LLM client for rewriting
        question_text: The original question to clarify
        user_message: The user's clarification request
        project_context: Business context for appropriate communication style
    """
    instr = (
        "CONTEXTO: VocÃª Ã© uma recepcionista brasileira profissional. A pessoa nÃ£o entendeu sua pergunta e pediu esclarecimento.\n\n"
        "RESPOSTA CONCISA E PROFISSIONAL:\n"
        "Construa UMA frase cordial que:\n"
        "- ReconheÃ§a gentilmente ('ah, claro', 'certo', 'sem problemas')\n"
        "- Reformule de forma mais clara e simples\n"
        "- MÃ¡ximo 1-2 linhas no WhatsApp\n"
        "- Tom: recepcionista simpÃ¡tica mas competente\n\n"
        "EXEMPLOS DE TOM ADEQUADO:\n"
        "- 'Claro! Preciso saber as dimensÃµes do galpÃ£o - comprimento e largura em metros'\n"
        "- 'Sem problemas! Qual o tamanho do espaÃ§o? Tipo 15x20 metros?'\n"
        "- 'Certo, deixa eu reformular: quantos metros tem de comprimento e largura?'\n\n"
        "CRÃTICO:\n"
        "- NÃƒO faÃ§a listas numeradas ou explicaÃ§Ãµes longas\n"
        "- NÃƒO seja muito casual ('E aÃ­', 'foi mal', 'ops')\n"
        "- NÃƒO seja muito formal ('Senhor/Senhora', 'poderia informar')\n"
        "- SÃ“ reformule a pergunta de forma clara e cordial\n"
        "- Tom de recepcionista brasileira profissional\n"
    )

    # Add project context to instruction if available
    if project_context and project_context.has_rewriter_context():
        context_prompt = project_context.get_rewriter_context_prompt()
        instr = f"{instr}\n{context_prompt}"

    try:
        text = f"Pergunta: {question_text}\nUsuÃ¡rio perguntou: {user_message}"
        rewritten = llm.rewrite(instr, text)
        
        # Log the prompt and response
        prompt_logger.log_prompt(
            prompt_type="clarify_reask",
            instruction=instr,
            input_text=text,
            response=rewritten if isinstance(rewritten, str) else str(rewritten),
            model=getattr(llm, 'model_name', 'unknown'),
            metadata={"has_project_context": project_context is not None}
        )
        
        if isinstance(rewritten, str) and rewritten.strip():
            first_line = next((ln.strip() for ln in rewritten.splitlines() if ln.strip()), "")
            if first_line.startswith(("- ", "* ")):
                first_line = first_line[2:].strip()
            if first_line.startswith('"') and first_line.endswith('"') and len(first_line) > 1:
                first_line = first_line[1:-1].strip()
            return first_line
        return question_text
    except Exception as e:
        # Log the error
        prompt_logger.log_prompt(
            prompt_type="clarify_reask_error",
            instruction=instr,
            input_text=text,
            response=f"ERROR: {e}",
            model=getattr(llm, 'model_name', 'unknown'),
            metadata={"error": str(e)}
        )
        return question_text


def rewrite_whatsapp_multi(
    llm: LLMClient,  # type: ignore[name-defined]
    original_text: str,
    chat_window: list[dict[str, str]] | None = None,
    *,
    max_followups: int = 2,
    project_context: ProjectContext | None = None,  # type: ignore[name-defined]
) -> list[dict[str, int | str]]:
    """Rewrite an assistant reply into human-like WhatsApp-style messages.

    Returns a list of {"text": str, "delay_ms": int} objects. The first message should have
    delay_ms=0. Follow-ups should be paced to feel human.

    If rewriting fails, returns a single message with original_text.
    """
    if not original_text.strip():
        return [{"text": original_text, "delay_ms": 0}]

    # Proceed to rewrite even for direct questions. Post-processing below preserves
    # the exact question wording and ensures a single question bubble.

    history_lines: list[str] = []
    last_user_message: str = ""
    for turn in chat_window or []:
        role = (turn.get("role") or "").strip()
        content = (turn.get("content") or "").strip()
        if role and content:
            history_lines.append(f"{role}: {content}")
            if role.lower() == "user":
                last_user_message = content

    history_block = "\n".join(history_lines[-200:])  # cap to keep prompt bounded

    instruction = (
        "VocÃª Ã© uma recepcionista brasileira profissional mas muito calorosa no WhatsApp. Imagine uma recepcionista exemplar - "
        "competente, simpÃ¡tica, acolhedora, que as pessoas adoram ser atendidas por ela, mas sempre mantÃ©m profissionalismo.\n\n"
        "ANÃLISE CONTEXTUAL (FAÃ‡A ISSO PRIMEIRO):\n"
        "Antes de responder, analise profundamente:\n"
        "1. O tom emocional da Ãºltima mensagem do usuÃ¡rio (animado? confuso? neutro? frustrado? curioso?)\n"
        "2. O contexto da conversa atÃ© agora (primeira interaÃ§Ã£o? jÃ¡ conversaram? qual o assunto?)\n"
        "3. O tipo de resposta necessÃ¡ria (informaÃ§Ã£o? confirmaÃ§Ã£o? escolha? esclarecimento?)\n"
        "4. A complexidade da resposta (simples = 1 msg, mÃ©dia = 2 msgs, complexa = 3+ msgs)\n\n"
        "PERSONALIDADE PROFISSIONAL CALOROSA:\n"
        "Adapte sua resposta baseado no contexto:\n"
        "- UsuÃ¡rio animado â†’ Seja cordial e positiva: 'Que Ã³timo!', 'Perfeito!', 'Excelente!'\n"
        "- UsuÃ¡rio confuso â†’ Seja paciente e clara: 'Sem problemas, vou esclarecer', 'Claro, deixa eu explicar'\n"
        "- UsuÃ¡rio neutro â†’ Seja profissional mas calorosa: 'Claro', 'Perfeito', 'Vamos lÃ¡'\n"
        "- UsuÃ¡rio apressado â†’ Seja eficiente mas gentil: 'Certo, vamos direto ao ponto'\n"
        "- Primeira interaÃ§Ã£o â†’ Seja acolhedora mas profissional: 'OlÃ¡! Como posso ajudar?'\n\n"
        "ESTRATÃ‰GIA DE MENSAGENS:\n"
        "Decida quantas mensagens baseado no CONTEÃšDO e CONTEXTO:\n\n"
        "â€¢ 1 MENSAGEM quando:\n"
        "  - Pergunta simples e direta (nome, horÃ¡rio, sim/nÃ£o)\n"
        "  - ConfirmaÃ§Ã£o rÃ¡pida\n"
        "  - UsuÃ¡rio parece apressado\n"
        "  Exemplo: pergunta 'Qual seu nome?' â†’ resposta 'Me chamo Ana! E vocÃª?'\n\n"
        "â€¢ 2 MENSAGENS quando:\n"
        "  - Precisa reconhecer + perguntar\n"
        "  - InformaÃ§Ã£o + confirmaÃ§Ã£o\n"
        "  - Criar conexÃ£o emocional + conteÃºdo\n"
        "  Exemplo: usuÃ¡rio confuso â†’ 'Ah, entendi sua dÃºvida!' + 'Ã‰ assim: [explicaÃ§Ã£o]'\n\n"
        "â€¢ 3+ MENSAGENS quando:\n"
        "  - ExplicaÃ§Ã£o em etapas\n"
        "  - MÃºltiplas opÃ§Ãµes para escolher\n"
        "  - HistÃ³ria ou contextualizaÃ§Ã£o\n"
        "  - UsuÃ¡rio muito engajado (merece atenÃ§Ã£o extra)\n"
        "  Exemplo: processo complexo â†’ 'Ok, vou te explicar!' + 'Primeiro, vocÃª...' + 'Depois Ã© sÃ³...'\n\n"
        "TÃ‰CNICAS DE NATURALIDADE PROFISSIONAL:\n"
        "- Varie inÃ­cios cordiais: 'Perfeito!', 'Ã“timo!', 'Certo!', 'Claro!', 'Beleza!', ou direto na pergunta\n"
        "- Use reticÃªncias com moderaÃ§Ã£o: 'entÃ£o...', 'Ã© que...', quando apropriado\n"
        "- Emojis moderados e profissionais: ðŸ˜Š (gentileza), quando fizer sentido no contexto\n"
        "- InterjeiÃ§Ãµes suaves: 'nÃ©?', 'certo?', quando couber naturalmente\n"
        "- ExpressÃµes brasileiras cordiais: 'que bom', 'perfeito', 'excelente', 'tranquilo'\n\n"
        "ADAPTAÃ‡ÃƒO INTELIGENTE:\n"
        "Observe padrÃµes do usuÃ¡rio e adapte adequadamente:\n"
        "- UsuÃ¡rio formal â†’ mantenha mais profissional\n"
        "- UsuÃ¡rio casual â†’ seja calorosa mas ainda cordial\n"
        "- UsuÃ¡rio Ã© breve â†’ seja concisa\n"
        "- UsuÃ¡rio elabora â†’ vocÃª pode desenvolver mais\n\n"
        "DECISÃ•ES IMPORTANTES:\n"
        "- Quando houver OPÃ‡Ã•ES: apresente cordialmente ('seria consulta, exame, ou outro serviÃ§o?' ao invÃ©s de menu numerado)\n"
        "- Para ESCLARECIMENTOS: reconheÃ§a gentilmente antes de esclarecer\n"
        "- Em CONFIRMAÃ‡Ã•ES: seja cordial ('Perfeito! Anotei aqui' ao invÃ©s de sÃ³ 'Ok')\n"
        "- Para INFORMAÃ‡Ã•ES: seja positiva ('Consegui um Ã³timo horÃ¡rio!' ao invÃ©s de sÃ³ 'HorÃ¡rio disponÃ­vel:')\n\n"
        "AUTENTICIDADE PROFISSIONAL:\n"
        "- Seja humana mas mantendo competÃªncia\n"
        "- Demonstre interesse genuÃ­no pelo cliente\n"
        "- Use diminutivos carinhosos quando apropriado: 'minutinho', 'rapidinho'\n"
        "- Mantenha sempre o equilÃ­brio: calorosa mas respeitosa\n\n"
        "Formato: JSON array [{\"text\": string, \"delay_ms\": number}]\n"
        "Delays: primeira sempre 0, outras entre 2200-3800ms (varie para parecer natural)\n"
        "Tamanho: mÃ¡ximo 150 caracteres por mensagem, mas varie (algumas bem curtas como 'Perfeito!' outras maiores)\n"
    )

    # Add project context to instruction if available
    if project_context and project_context.has_rewriter_context():
        context_prompt = project_context.get_rewriter_context_prompt()
        instruction = f"{instruction}\n{context_prompt}"

    payload = (
        f"Original assistant reply:\n{original_text}\n\n"
        f"Latest user message (if any):\n{last_user_message}\n\n"
        f"Conversation window (oldest to newest):\n{history_block}"
    )

    try:
        raw = llm.rewrite(instruction, payload)
        
        # Log the prompt and response
        prompt_logger.log_prompt(
            prompt_type="whatsapp_multi",
            instruction=instruction,
            input_text=payload,
            response=raw if isinstance(raw, str) else str(raw),
            model=getattr(llm, 'model_name', 'unknown'),
            metadata={
                "has_project_context": project_context is not None,
                "max_followups": max_followups,
                "last_user_message": last_user_message
            }
        )
        
        # Try to parse JSON array
        import json

        messages = json.loads(raw) if isinstance(raw, str) else []
        out: list[dict[str, int | str]] = []
        if isinstance(messages, list):
            for i, item in enumerate(messages):
                if not isinstance(item, dict):
                    continue
                text = str(item.get("text") or "").strip()
                if not text:
                    continue
                try:
                    delay_ms_val = int(item.get("delay_ms", 0))
                except Exception:
                    delay_ms_val = 0 if i == 0 else MIN_FOLLOWUP_DELAY_MS
                if i == 0:
                    delay_ms_val = 0
                out.append({"text": text, "delay_ms": max(0, delay_ms_val)})
                # Soft cap to prevent pathological outputs; keep generous to feel free-form
                if len(out) >= MAX_MULTI_MESSAGES:
                    break
        if out:
            # Normalize follow-up delays into human pacing range
            for idx in range(1, len(out)):
                try:
                    d = int(out[idx].get("delay_ms", MIN_FOLLOWUP_DELAY_MS))
                except Exception:
                    d = MIN_FOLLOWUP_DELAY_MS
                if d < MIN_FOLLOWUP_DELAY_MS:
                    d = MIN_FOLLOWUP_DELAY_MS
                elif d > MAX_FOLLOWUP_DELAY_MS:
                    d = MAX_FOLLOWUP_DELAY_MS
                out[idx]["delay_ms"] = d
            return out
    except Exception:
        # Fall through to deterministic fallback below
        pass

    # Simple fallback: if LLM fails, return original text as-is
    return [{"text": original_text, "delay_ms": 0}]
